apiVersion: v1
kind: Service
metadata:
  name: connect
  labels:
    app: connect
spec:
  ports:
    - port: 8083
      name: connect
  selector:
    app: connect
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"kafka"},{"apiVersion":"apps/v1","kind":"Deployment","name":"mongodb"},{"apiVersion":"apps/v1","kind":"Deployment","name":"mysql"}]'
  name: connect
  labels:
    app: connect
    app.kubernetes.io/part-of: demo-infra
spec:
  selector:
    matchLabels:
      app: connect
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: connect
    spec:
      containers:
      - image: quay.io/rhdevelopers/connect-image-eda-cep-pattern:v23.02
        name: connect
        command: [
          "sh", "-c",
          "bin/connect-distributed.sh config/connect-distributed.properties"
        ]
        env:
          - name: LOG_DIR
            value: /tmp/logs
        volumeMounts:
            - name: config
              mountPath: /opt/kafka/config/
      volumes:
        - name: config
          configMap:
            name: connect-config
---
apiVersion: batch/v1
kind: Job
metadata:
  name: connectors-creator
  labels:
    app: connectors-creator
    app.kubernetes.io/part-of: demo-infra
spec:
  template:
    spec:
      containers:
      - name: connectors-creator
        image: quay.io/debezium/tooling:1.2
        command: [
          "bash","/home/register_connectors.sh"
        ]
        volumeMounts:
            - name: connectors-creator
              mountPath: /home

      restartPolicy: OnFailure
      volumes:
        - name: connectors-creator
          configMap:
            name: connectors-config
  backoffLimit: 10
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: connect-config
  labels:
    app: connect
data:
  connect-log4j.properties: |
    log4j.rootLogger=INFO, stdout, connectAppender

    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout

    log4j.appender.connectAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.connectAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.connectAppender.File=${kafka.logs.dir}/connect.log
    log4j.appender.connectAppender.layout=org.apache.log4j.PatternLayout

    connect.log.pattern=[%d] %p %X{connector.context}%m (%c:%L)%n

    log4j.appender.stdout.layout.ConversionPattern=${connect.log.pattern}
    log4j.appender.connectAppender.layout.ConversionPattern=${connect.log.pattern}

    log4j.logger.org.apache.zookeeper=ERROR
    log4j.logger.org.reflections=ERROR

  connect-distributed.properties: |
    bootstrap.servers=kafka:9092
    group.id=connect-cluster

    key.converter=org.apache.kafka.connect.json.JsonConverter
    value.converter=org.apache.kafka.connect.json.JsonConverter
    
    key.converter.schemas.enable=true
    value.converter.schemas.enable=true

    offset.storage.topic=connect-offsets
    offset.storage.replication.factor=1
    config.storage.topic=connect-configs
    config.storage.replication.factor=1

    status.storage.topic=connect-status
    status.storage.replication.factor=1
    
    offset.flush.interval.ms=10000
    plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: connectors-config
  labels:
    app: connect
data:
  register_connectors.sh: |
    #!/bin/bash
    echo "waiting for kafka connect to start listening..."
    while [ $(curl -s -o /dev/null -w %{http_code} http://connect:8083/connectors) -eq 000 ] ; do 
        echo -e $(date) " kafka connect listener HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://connect:8083/connectors) " (waiting for 200)"
        sleep 2
    done
    echo "kafka connect is up!"
    sleep 8
    curl -X POST -H "Content-Type: application/json" -d @/home/create_mysql_dbz_source_connector.json http://connect:8083/connectors
    curl -X POST -H "Content-Type: application/json" -d @/home/create_mongodb_sink_connector.json http://connect:8083/connectors
    echo "done registering connectors"

  create_mongodb_sink_connector.json: |
    {
      "name": "enriched-iot-mongodb-sink-001",
      "config": {
          "topics": "iot-enriched",
          "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
          "key.converter": "org.apache.kafka.connect.converters.IntegerConverter",
          "key.converter.schemas.enable":false,
          "value.converter": "org.apache.kafka.connect.json.JsonConverter",
          "value.converter.schemas.enable":false,
          "tasks.max": "1",
          "connection.uri":"mongodb://mongodb:27017",
          "database":"iot_db",
          "collection":"sensors-ts",
          "timeseries.timefield":"timestamp",
          "timeseries.timefield.auto.convert":"true",
          "timeseries.timefield.auto.convert.date.format": "yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSS][SS][S]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSS][SS][S]][ ]X][HH:mm:ss[[.][SSS][SS][S]]]",
          "timeseries.metafield":"deviceId",
          "timeseries.granularity":"seconds"
      }
    }

  create_mysql_dbz_source_connector.json: |
    {
      "name": "iot-device-data-source-001",
      "config": {
          "connector.class": "io.debezium.connector.mysql.MySqlConnector",
          "key.converter": "org.apache.kafka.connect.converters.IntegerConverter",
          "key.converter.schemas.enable": false,
          "value.converter": "org.apache.kafka.connect.json.JsonConverter",
          "value.converter.schemas.enable": false,
          "tasks.max": "1",
          "database.hostname": "mysql",
          "database.port": "3306",
          "database.user": "root",
          "database.password": "debezium",
          "database.server.id": "12345",
          "database.include.list": "iot_db",
          "table.include.list": "iot_db.devices",
          "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
          "schema.history.internal.kafka.topic": "schemachanges.iot_db",
          "topic.prefix": "mysql",
          "transforms": "extractId,unwrap",
          "transforms.extractId.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
          "transforms.extractId.field": "id",
          "transforms.extractId.predicate": "isPayloadTopic",
          "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
          "transforms.unwrap.predicate": "isPayloadTopic",
          "predicates": "isPayloadTopic",
          "predicates.isPayloadTopic.type": "org.apache.kafka.connect.transforms.predicates.TopicNameMatches",
          "predicates.isPayloadTopic.pattern": ".*devices",
          "include.schema.changes": false
      }
    }
